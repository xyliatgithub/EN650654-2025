{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab part 2: Adversarial Attacks Against LLM-Based Spam Filters**\n"
      ],
      "metadata": {
        "id": "iKDcdwsGj0vi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "\n",
        "\n",
        "In this lab, we attack several LLM-enabled spam filters by adversarial emails leveraging the *magic words* identified with direct access to another model. This way of attacking the LLM-enabled spam filters is called black-box attack.\n",
        "\n",
        "\n",
        "Specifically, the tasks in this lab:\n",
        "- Add the magic words or sentences made from these words to spam emails.\n",
        "- Evaluate the effectiveness of these adversarial emails against large language model (LLM)-based spam detection systems.\n",
        "- Analyze how different insertion positions within the email body affect the attack success rate.\n",
        "\n",
        "Through this experiment, we seek to gain insights into the vulnerabilities of modern spam filters powered by LLMs. The following reference contains more information on relevant works:\n",
        "\n",
        "Q. Tang and X. Li, “WiP: An Investigation of Large Language Models and Their Vulnerabilities in Spam Detection,” The Hot Topics in the Science of Security Symposium (HotSoS 2025), Virtual Event, April 1-3, 2025. [Download](https://isi.jhu.edu/wp-content/uploads/2025/03/HoTSoS_LLM_Based_Spam_Detection.pdf)\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "For improved performance, you can switch the runtime type to GPU. To do this, click on the arrow icon in the upper-right corner of the notebook interface and select **\"Change runtime type\"**, then choose **GPU** as the hardware accelerator.\n",
        "\n",
        "**Acknowledgement**\n",
        "\n",
        "The code is adapted and extended from the following work:\n",
        "\"Natural language processing with GPT models,\" Github: https://github.com/SiavashShams/Spam_detection_GPT2\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8UI9JYAaVCWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Loading Dependencies**\n"
      ],
      "metadata": {
        "id": "AcAxrnjuW4we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics #install torchmetrics package, after installation you need to restart your session"
      ],
      "metadata": {
        "id": "G8JB0TUavexa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount your drive:\n"
      ],
      "metadata": {
        "id": "OVgsDUUrwRLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3gUz4NBgZrYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading dependencies for the BERT LLM model:"
      ],
      "metadata": {
        "id": "J_zpptvzwVcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from tqdm import trange"
      ],
      "metadata": {
        "id": "YaMrNKRFYdyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading dependencies for the GPT-2 LLM model:\n",
        "\n"
      ],
      "metadata": {
        "id": "efWg7vEKwaKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, GPT2Config\n",
        "from tabulate import tabulate\n",
        "import random\n",
        "from torchmetrics.classification import Recall, Accuracy, AUROC, Precision\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "8_3VIvEHxYXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Loading and Splitting the Dataset**\n",
        "\n",
        "We begin by loading the email dataset and splitting it into training and validation sets, using an 80/20 split. We do not need to have three subsets as we only train one classifier for testing and attack.\n",
        "\n",
        "- **Training Dataset**: Used to train our LLM-based spam filter model.\n",
        "\n",
        "- **Validation Dataset**: Used to evaluate the performance of the trained spam filter. This is similar to the **test set** for the lab tasks related to the SVM classifier.\n",
        "\n",
        "Additionally, we **randomly reserve 10 spam emails** from the validation set. These emails will be modified using magic words to generate adversarial examples. We use only these 10 spam emails for the attack so you can manually inject the magic words or sentences in them if you choose so.\n"
      ],
      "metadata": {
        "id": "rh-KW3d8XdeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def data_extraction():\n",
        "\n",
        "  # Change the 'messages.csv' to the filename you uploaded.\n",
        "  df = pd.read_csv('/content/drive/MyDrive/messages.csv')\n",
        "\n",
        "  x = df.message\n",
        "  y = df.label\n",
        "\n",
        "  x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=99, stratify=y)\n",
        "\n",
        "  spam_emails = x_val[y_val == 1]\n",
        "  reserved_samples = spam_emails.sample(n=10, random_state=2025)\n",
        "  reserved_samples.to_csv(\"reserved_samples.csv\", index=False, header=True)\n",
        "\n",
        "  return x_train, x_val, y_train, y_val, reserved_samples\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels, reserved_samples = data_extraction()\n",
        "print(train_inputs.shape, validation_inputs.shape)\n",
        "\n",
        "# Display reserved samples\n",
        "for i, email in enumerate(reserved_samples.tolist()):\n",
        "    print(f\"Sample {i+1}: {email}\\n\")\n"
      ],
      "metadata": {
        "id": "d_NPSNKqomkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Preprocessing the Data**\n",
        "\n",
        "In this section, we define the preprocessing pipelines tailored for BERT and GPT-2 models.\n",
        "\n",
        "- **For BERT**:  \n",
        "  The text is tokenized, converted into token IDs, padded or truncated to a fixed sequence length, and corresponding attention masks are generated. These steps are necessary to match BERT's input requirements.\n",
        "\n",
        "- **For GPT-2**:  \n",
        "  The text is tokenized and converted into token IDs, then processed as continuous sequences. Unlike BERT, GPT-2 relies less on padding and does not require explicit attention masks in most cases.\n",
        "\n",
        "These preprocessing steps ensure that the input data is properly formatted for efficient and effective training and inference with each respective model.\n"
      ],
      "metadata": {
        "id": "zLcWQsGoczOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(input_text, tokenizer):\n",
        "    '''\n",
        "    Returns <class transformers.tokenization_utils_base.BatchEncoding>\n",
        "    '''\n",
        "    return tokenizer(\n",
        "        input_text,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=32,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "# Function to preprocess data for BERT\n",
        "def preprocessing_for_bert(inputs, labels, tokenizer=bert_tokenizer):\n",
        "    '''\n",
        "    data: Pandas dataframe containing data and their labels.\n",
        "    Returns list of 2D tensors.\n",
        "    '''\n",
        "    encoding_dict = preprocessing(inputs.tolist(), tokenizer)\n",
        "    token_id = encoding_dict['input_ids']\n",
        "    attention_masks = encoding_dict['attention_mask']\n",
        "    labels = torch.tensor(labels.tolist())\n",
        "    return token_id, attention_masks, labels\n",
        "\n",
        "# Load the GPT2 tokenizer\n",
        "gpt_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", add_prefix_space=True)\n",
        "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token\n",
        "\n",
        "# Remember we want to use the last token's embedding to represent the entire sentence\n",
        "gpt_tokenizer.padding_side = 'left'\n",
        "\n",
        "def preprocessing_for_GPT(inputs, labels, tokenizer=gpt_tokenizer):\n",
        "    '''\n",
        "    data: Pandas dataframe containing data and their labels.\n",
        "    Returns list of 2D tensors.\n",
        "    '''\n",
        "    encoding_dict = preprocessing(inputs.tolist(), tokenizer)\n",
        "    token_id = encoding_dict['input_ids']\n",
        "    attention_masks = encoding_dict['attention_mask']\n",
        "    labels = torch.tensor(labels)\n",
        "    return token_id, attention_masks, labels\n",
        "\n"
      ],
      "metadata": {
        "id": "wZ-BIOvBzE0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Training LLM Spam Filters and Evaluating Adversarial Attack**\n",
        "First, we train spam filters using BERT and GPT-2. The training process involves fine-tuning the pre-trained models using labeled spam and ham email. Note that each training process generates two classifier using two epochs. Each classifier is evaluated on the validation dataset for their performance.\n",
        "\n",
        "Once the models are trained, we save them for later evaluation. In subsequent steps, we can load the saved models to evaluate their performance, including testing their robustness against adversarial attacks to evade detection. This allows efficient re-use of the trained models without retraining each time."
      ],
      "metadata": {
        "id": "q8zBfr5QX_Fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(1) Train Bert-based Spam Filters**"
      ],
      "metadata": {
        "id": "uhUYyld6lCNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the training dataset for bert\n",
        "train_token_id, train_attention_masks, train_labels = preprocessing_for_bert(train_inputs, train_labels, bert_tokenizer)\n",
        "# preprocess the validation dataset for bert\n",
        "validation_token_id, validation_attention_masks, validation_labels = preprocessing_for_bert(validation_inputs, validation_labels, bert_tokenizer)\n",
        "print(train_token_id.shape, validation_token_id.shape)\n",
        "\n",
        "# DataLoader initialization\n",
        "batch_size = 16\n",
        "train_data = TensorDataset(train_token_id, train_attention_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_token_id, validation_attention_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "QlZx3uXy4G0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.to(device)\n",
        "\n",
        "# Training setup\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "epochs = 2\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_steps = 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Clear out gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
        "            logits = outputs.logits\n",
        "            predicted_labels = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_labels.extend(b_labels.cpu().numpy())\n",
        "        all_preds.extend(predicted_labels.cpu().numpy())\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n",
        "    recall = recall_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n",
        "\n",
        "    # Calculate False Positive Rate (FPR) and False Negative Rate (FNR)\n",
        "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Epoch {_ + 1}/{epochs}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
        "    print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n",
        "    print(\"\\n\\t - Train loss: {:.4f}\".format(tr_loss / nb_tr_steps))\n",
        "\n",
        "    # save model\n",
        "    model_path = f\"/content/drive/My Drive/CID_final/Bert_epoch{_ + 1}.pth\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n"
      ],
      "metadata": {
        "id": "1UYTSRv_bUlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(2) Train GPT-2-based Spam Filters**"
      ],
      "metadata": {
        "id": "__dp3XRJlKst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the training dataset for gpt2\n",
        "train_token_id, train_attention_masks, train_labels = preprocessing_for_GPT(train_inputs, train_labels, gpt_tokenizer)\n",
        "# preprocess the validation dataset for gpt2\n",
        "validation_token_id, validation_attention_masks, validation_labels = preprocessing_for_GPT(validation_inputs, validation_labels, gpt_tokenizer)\n",
        "print(train_token_id.shape, validation_token_id.shape)\n",
        "\n",
        "# DataLoader initialization\n",
        "batch_size = 16\n",
        "train_data = TensorDataset(train_token_id, train_attention_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_token_id, validation_attention_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "eCs3f1VwC90D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = GPT2ForSequenceClassification.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    num_labels=2,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False\n",
        ")\n",
        "model.resize_token_embeddings(len(gpt_tokenizer))\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "model.to(device)\n",
        "# Training setup\n",
        "from torch import optim\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "# Use torchmetrics to set up accuracy, recall, precision, and auroc\n",
        "accuracy = Accuracy(task='binary')\n",
        "recall = Recall(task='binary')\n",
        "precision = Precision(task='binary')\n",
        "auroc = AUROC(task='binary',num_classes=2)\n",
        "# Assuming the following imports and initializations\n",
        "from torchmetrics import AUROC\n",
        "auroc = AUROC(task='binary',num_classes=2)  # Initialized outside the loop\n",
        "\n",
        "epochs = 2\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Clear out gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss = train_output.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
        "            logits = outputs.logits\n",
        "            predicted_labels = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_labels.extend(b_labels.cpu().numpy())\n",
        "        all_preds.extend(predicted_labels.cpu().numpy())\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n",
        "    recall = recall_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n",
        "\n",
        "    # Calculate False Positive Rate (FPR) and False Negative Rate (FNR)\n",
        "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Epoch {_ + 1}/{epochs}\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
        "    print(f\"False Negative Rate (FNR): {fnr:.4f}\")\n",
        "    print(\"\\n\\t - Train loss: {:.4f}\".format(tr_loss / nb_tr_steps))\n",
        "    # save model\n",
        "    model_path = f\"/content/drive/My Drive/CID_final/GPT_epoch{_ + 1}.pth\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n"
      ],
      "metadata": {
        "id": "nCj1CtGyCcNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Adversarial Attack Using Magic Words/Sentences**\n",
        "The attacks apply different insertion strategies to the selected 10 spam emails to find out the attack success rate. You can manually do the insertion or write code to do it.\n",
        "\n",
        "- For word based insertion, insert your magic words as a string.\n",
        "  - word_0: insert at the begining of the email\n",
        "  - word_1: insert behind the first sentence\n",
        "  - word_2: insert behind the second sentence\n",
        "  - word_3: insert behind the third sentence\n",
        "  - word_∞: insert at the end of the email\n",
        "- For sentence based insertion, insert your magic sentences. (You need to create one or two semantically meaningful sentences from these words.)\n",
        "  - sentence_0: insert at the begining of the email\n",
        "  - sentence_1: insert behind the first sentence\n",
        "  - sentence_2: insert behind the second sentence\n",
        "  - sentence_3: insert behind the third sentence\n",
        "  - sentence_∞: insert at the end of the email"
      ],
      "metadata": {
        "id": "y2FVzJx8YKNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is a helper function for this task if you like to use code to modify the ten spam emails.\n",
        "\n",
        "**Hint:** Save the adversarial emails after modification as a file so you can use the same code to load these emails for attacks in the next section."
      ],
      "metadata": {
        "id": "qweTJ_GuBOhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_procession(text, insertion, position):\n",
        "    punctuation = ['.', '!', '?']\n",
        "    punctuation_indices = [i for i, char in enumerate(text) if char in punctuation]\n",
        "\n",
        "    if position == \"sentence_0\" or position == \"word_0\":\n",
        "        return insertion + text\n",
        "    elif position == \"sentence_∞\" or position == \"word_∞\":\n",
        "        return text + insertion\n",
        "    else:\n",
        "        position_index = int(position.split('_')[1]) - 1\n",
        "        if position_index < len(punctuation_indices):\n",
        "            insert_pos = punctuation_indices[position_index] + 1\n",
        "            text = text[:insert_pos] + \" \" + insertion + text[insert_pos:]\n",
        "    return text\n",
        "\n",
        "def insert_magic_word(text, magic_word, magic_sentences, position):\n",
        "  if \"word\" in position:\n",
        "    return insert_procession(text, magic_word, position)\n",
        "  elif \"sentence\" in position:\n",
        "    return insert_procession(text, magic_sentences, position)"
      ],
      "metadata": {
        "id": "f5MCn6XHjOb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1:**\n",
        "\n",
        "Please list the magic words and sentence(s) you use. Please describe how you generate the adversarial email using these attack tokens.\n",
        "\n"
      ],
      "metadata": {
        "id": "JhpH8tAMfSGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Evaluating the Attack Effectiveness**\n",
        "\n",
        "In this section, we measure the effectiveness of adversarial attacks against different LLM-based spam filters by measuring their **attack success rate** - **False Negative Rate (FNR)** in the evaluation performance.\n",
        "\n",
        "Specifically, we assess how well the adversarial emails evade detection when adversarial tokens (magic words or sentences) are inserted at various positions within the email text."
      ],
      "metadata": {
        "id": "_HKy2i8TYR8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(1) Test and Attack BERT-based Spam Filters**\n",
        "\n",
        "Note that for the attacks, you should load the correct spam emails. Make sure you replace \"reserved_samples.csv\" as needed."
      ],
      "metadata": {
        "id": "6-8nRF479Mje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(2):\n",
        "  # evaluate your result on Bert models\n",
        "  # load model\n",
        "  model_path = f\"/content/drive/My Drive/CID_final/Bert_epoch{_ + 1}.pth\"\n",
        "  model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "  model.load_state_dict(torch.load(model_path))\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  print(f\" Bert model epoch{_ +1} is loaded.\")\n",
        "  batch_size = 10\n",
        "\n",
        "  reserved_samples = pd.read_csv(\"reserved_samples.csv\")\n",
        "  labels = pd.Series([1]*10) # all \"1\" spam\n",
        "  reserved_token_id, reserved_attention_masks, reserved_labels = preprocessing_for_bert(reserved_samples['message'], labels, bert_tokenizer)\n",
        "  reserved_data = TensorDataset(reserved_token_id, reserved_attention_masks, reserved_labels)\n",
        "  reserved_sampler = SequentialSampler(reserved_data)\n",
        "  reserved_dataloader = DataLoader(reserved_data, sampler=reserved_sampler, batch_size=batch_size)\n",
        "\n",
        "  all_preds = []\n",
        "  all_labels = [] # should be all \"1\" spam\n",
        "  for batch in reserved_dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      with torch.no_grad():\n",
        "          outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
        "          logits = outputs.logits\n",
        "          predicted_labels = torch.argmax(logits, dim=1)\n",
        "      all_preds.extend(predicted_labels.cpu().numpy())\n",
        "      all_labels.extend(b_labels.cpu().numpy())\n",
        "\n",
        "  accuracy = accuracy_score(all_labels, all_preds)\n",
        "  precision = precision_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n",
        "  recall = recall_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n",
        "  f1 = f1_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n",
        "\n",
        "  # Calculate False Positive Rate (FPR) and False Negative Rate (FNR)\n",
        "  tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
        "  fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "  fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "\n",
        "  # Print metrics\n",
        "   print(f\"Accuracy: {accuracy:.4f}\")\n",
        "  print(f\"Precision: {precision:.4f}\")\n",
        "  print(f\"Recall: {recall:.4f}\")\n",
        "  print(f\"F1 Score: {f1:.4f}\")\n",
        "  print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
        "  print(f\"False Negative Rate (FNR): {fnr:.4f}\")"
      ],
      "metadata": {
        "id": "jiDxKtKuiZFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **(2) Test and Attack GPT-2-based Spam Filters**\n",
        "\n",
        "Note that for the attacks, you should load the correct spam emails. Make sure you replace \"reserved_samples.csv\" as needed."
      ],
      "metadata": {
        "id": "antNxkZY8coo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(2):\n",
        "  # evaluate your result on GPT models\n",
        "  # load model\n",
        "  model_path = f\"/content/drive/My Drive/CID_final/GPT_epoch{_ +1}.pth\"\n",
        "  model = GPT2ForSequenceClassification.from_pretrained(\n",
        "      \"gpt2\",\n",
        "      num_labels=2,\n",
        "      output_attentions=False,\n",
        "      output_hidden_states=False\n",
        "  )\n",
        "  model.resize_token_embeddings(len(gpt_tokenizer))\n",
        "  model.config.pad_token_id = model.config.eos_token_id\n",
        "  model.load_state_dict(torch.load(model_path))\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  print(f\"GPT model epoch{_ +1} is loaded.\")\n",
        "  batch_size = 10\n",
        "\n",
        "  reserved_samples = pd.read_csv(\"reserved_samples.csv\")\n",
        "  labels = pd.Series([1]*10) # all \"1\" spam\n",
        "  reserved_token_id, reserved_attention_masks, reserved_labels = preprocessing_for_GPT(reserved_samples['message'], labels, gpt_tokenizer)\n",
        "  reserved_data = TensorDataset(reserved_token_id, reserved_attention_masks, reserved_labels)\n",
        "  reserved_sampler = SequentialSampler(reserved_data)\n",
        "  reserved_dataloader = DataLoader(reserved_data, sampler=reserved_sampler, batch_size=batch_size)\n",
        "\n",
        "  all_preds = []\n",
        "  all_labels = [] # should be all \"1\" spam\n",
        "  for batch in reserved_dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      with torch.no_grad():\n",
        "          outputs = model(input_ids=b_input_ids, attention_mask=b_input_mask)\n",
        "          logits = outputs.logits\n",
        "          predicted_labels = torch.argmax(logits, dim=1)\n",
        "      all_preds.extend(predicted_labels.cpu().numpy())\n",
        "      all_labels.extend(b_labels.cpu().numpy())\n",
        "\n",
        "  accuracy = accuracy_score(all_labels, all_preds)\n",
        "  precision = precision_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n",
        "  recall = recall_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n",
        "  f1 = f1_score(all_labels, all_preds, average=\"binary\", zero_division=1)\n",
        "\n",
        "  # Calculate False Positive Rate (FPR) and False Negative Rate (FNR)\n",
        "  tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
        "  fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "  fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
        "\n",
        "  # Print metrics\n",
        "  print(f\"Before modification\")\n",
        "  print(f\"Accuracy: {accuracy:.4f}\")\n",
        "  print(f\"Precision: {precision:.4f}\")\n",
        "  print(f\"Recall: {recall:.4f}\")\n",
        "  print(f\"F1 Score: {f1:.4f}\")\n",
        "  print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
        "  print(f\"False Negative Rate (FNR): {fnr:.4f}\")"
      ],
      "metadata": {
        "id": "T3SpD0UpL8r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2**\n",
        "\n",
        "(1) Please generate plots of the success rate results for different insertion methods (word/senetence) and insertion positions on different LLM spam filters with different training epochs. Please label the plots (e.g., the meaning of x and y axis) clearly. You can have multiple charts if needed.\n",
        "\n",
        "(2) What are you obervations (at least two important ones) from these results? Can you explain these obeservations?\n",
        "\n",
        "Hint: Think about the differences that injection positions, LLM models, and epochs make."
      ],
      "metadata": {
        "id": "Rh2YhZLzmLLp"
      }
    }
  ]
}