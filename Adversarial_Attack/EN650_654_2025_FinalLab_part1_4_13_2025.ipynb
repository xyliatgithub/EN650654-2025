{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBdiD49e5Q7L"
      },
      "source": [
        "# **Lab part 1: Adversarial Attacks against Machine Learning-Based Spam Filters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSubQvScyKdQ"
      },
      "source": [
        "## **Introduction**\n",
        "Machine learning-based spam detection models learn from a set of labeled email. One class of vulnerabilities in machine learning models can allow an attack to modify malicious instances to generate adversarial examples. These modified instances may bypass a trained model, e.g., a support vector machine (SVM) classifier used here, without being detected. This is usually done by adding small changes, called perturbations, to the feature vector that the model processes. However, feature extraction methods can make it difficult to translate such numerical perturbation in the feature space, to needed changes to an email consisting of words in the problem space. After all, you can only add or remove a word as whole in the spam scenario of computer security.\n",
        "\n",
        "This lab uses a statistical method to understand how creating adversarial examples purposely modifies a TF-IDF (term frequency-inverse document frequency) feature vector representing an email. A set of \"magic words\" are identified by examining the TF-IDF features to look for those that experience the most significant changes made by a Projected Gradient Descent (PGD) algorithm. These words function in a similar way as in [good word attack](https://www.ceas.cc/papers-2005/125.pdf) previoouly studied. Adding these magic words to a spam email increases the chance of it to pass through the SVM classifier without being filtered out.  \n",
        "\n",
        "## **Publications**\n",
        "\n",
        "For more information on this method, you can refer to the following publications:\n",
        "\n",
        "(1) C. Wang, D. Zhang, S. Huang, X. Li, and L. Ding, “Crafting Adversarial Email Content against Machine Learning Based Spam Email Detection,” In Proceedings of the 2021 International Symposium on Advanced Security on Software and Systems (ASSS ’21) with AsiaCCS 2021, Virtual Event, Hong Kong, June 7, 2021. [Download](https://isi.jhu.edu/wp-content/uploads/2021/04/ASSS_Workshop_Paper.pdf)\n",
        "\n",
        "(2) Q. Cheng, A. Xu, X. Li, and L. Ding, “Adversarial Email Generation against Spam Detection Models through Feature Perturbation,” The 2022 IEEE International Conference on Assured Autonomy (ICAA’22), Virtual Event, March 22-23, 2022. [Download](https://isi.jhu.edu/wp-content/uploads/2022/04/Adversarial_Attacks_Against_Machine_Learning_Based_SpamFilters__IEEE.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv-y6Ac6FYWO"
      },
      "source": [
        "## **1. Loading Dataset**\n",
        "The dataset to be used is called Ling-Spam. The Ling-Spam dataset is a collection of 2,893 spam and ham email messages curated from the Linguist List. These messages focus on linguistic interests around job postings, research opportunities, and software discussion. You can download this dataset below coming with the lab assignment.\n",
        "\n",
        "### Acknowledgements\n",
        "The dataset and its information come from the original authors of \"A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists\".\n",
        "\n",
        "**Run the code block below:**\n",
        "\n",
        "choose the message.csv to upload. Wait until it shows 100% before you continue. The below code is for the \"Google Colab\" environment, for another environment (like Jupyter Notebook), you can choose the corresponding upload function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1re2gZWrle_P"
      },
      "source": [
        "**Run the code block below:**\n",
        "\n",
        "This splits the loaded dataset into three subsets of 64% training, 16% validation, and 20% testing set:\n",
        "- **Training Dataset** will be used to train SVM filter used to identify magic words\n",
        "- **Validation Dataset** will be used to identify magic words\n",
        "- **Test Dataset** will be used to evaluate model performance\n",
        "\n",
        "Notice that after identifying magic words, we will train another model to test our attack, which will use a 80% combined training dataset combining both the training and validation dataset here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZvxGJk-rkkc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def data_extraction():\n",
        "\n",
        "  # Change the 'messages.csv' to the filename you uploaded.\n",
        "  # df = pd.read_csv('messages.csv')\n",
        "  df = pd.read_csv('messages.csv')\n",
        "\n",
        "  x = df.message\n",
        "  y = df.label\n",
        "  # We first separate the entire dataset to 80% and 20%.\n",
        "  # Let the 80% of entire dataset becoming the first dataset(which will be split to traning dataset and the validation dataset), and let the 20% of entire dataset becoming the testing dataset.\n",
        "  x_train_val, x_test, y_train_val, y_test = train_test_split(x, y, test_size=0.2, random_state=99, stratify=y)\n",
        "  # Let the 80% of the train_val dataset be the traning dataset, and the 20% of the train_val dataset be the validation dataset.\n",
        "  x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.2, random_state=99, stratify=y_train_val)\n",
        "\n",
        "  return x_train, x_val, x_test, y_train, y_val, y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2aXVdK7DLuy",
        "outputId": "4b2b66bb-75a4-49d6-cb1d-801adfee250f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2345    crossing boundaries : interdisciplinary approa...\n",
            "2664    this is not spam ; you are receiving this mess...\n",
            "1049    computational aspects of cognitive science nsf...\n",
            "1560    dear subscribers : the linguist list has just ...\n",
            "2167    i would like to know of sources for bengali so...\n",
            "                              ...                        \n",
            "2598    = 20 the virtual girlfriend and virtual boyfri...\n",
            "1310    learn to put angels to work ! angels are anoth...\n",
            "1906    call for papers sixth workshop on very large c...\n",
            "2286    for some time , i have been puzzled by a claim...\n",
            "191                       how to get on elsnet ? thanks\\n\n",
            "Name: message, Length: 1851, dtype: object\n",
            "(1851,) (463,) (579,)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_val, X_test, Y_train, Y_val, Y_test = data_extraction()\n",
        "print(X_train)\n",
        "print(X_train.shape, X_val.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFUv7AmsFW8e"
      },
      "source": [
        "In the code block above, we have read the dataset into variables x\n",
        "and y. Variable x contains the email body in a list of words and variable y contains the class labels with 0 being ham and 1 being spam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w-WXw69wAT6"
      },
      "source": [
        "As noted before, we divide the entire data set randomly into three different data sets which are training data, validation data, and testing data. After we split the dataset twice: 64% of the entire dataset is the traning dataset(Y_train), 16% the validation dataset(X_val), and 20% the testing dataset(X_test).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCGLR9neo8zL"
      },
      "source": [
        "### **Question 1**\n",
        "\n",
        "Why do we need the three data subsets described above? Please explain what is a training dataset, a validation dataset, and a testing dataset. For some additional insights, you can refer to the article at https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7. (However, note that the validation dataset is used to identify the magical words in this lab.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KJ7iGauJih3"
      },
      "source": [
        "## **2. Preprocessing the Emails**\n",
        "For preparation for use, we remove all the HTML tags, numbers, punctuation marks, and English stop words to keep only useful information. We also convert all the words to lowercase and each paragraph into a single line instead of multiple lines. In the last step of preprocessing, we conduct stemming on all the words.\n",
        "\n",
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfS6VpTaH7Wu",
        "outputId": "3e5757ff-6541-451d-85ac-c81eadaee5f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/yihe/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/yihe/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /Users/yihe/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /Users/yihe/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# Remove the hyperlink.\n",
        "def remove_hyperlink(word):\n",
        "\n",
        "    return re.sub(r\"http\\S+\", \" \", word)\n",
        "\n",
        "\n",
        "# Convert the letter to lowercase.\n",
        "def to_lower(word):\n",
        "    result = word.lower()\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# Remove the numbers.\n",
        "def remove_number(word):\n",
        "    result = re.sub(r'\\d+', ' ', word)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# Remove the puncturations.\n",
        "def remove_punctuation(word):\n",
        "    result = word.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# Remove the whitespace.\n",
        "def remove_whitespace(word):\n",
        "    result = word.strip()\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# Merge multiple lines into one line.\n",
        "def replace_newline(word):\n",
        "\n",
        "    return word.replace('\\n', ' ')\n",
        "\n",
        "\n",
        "def clean_up_pipeline(sentence):\n",
        "    # Ensure the input is a string\n",
        "    if not isinstance(sentence, str):\n",
        "        sentence = str(sentence)\n",
        "\n",
        "    cleaning_utils = [remove_hyperlink, replace_newline, to_lower, remove_number, remove_punctuation, remove_whitespace]\n",
        "    for o in cleaning_utils:\n",
        "        sentence = o(sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "# Remove the stopwords, for example: a, and, an, above, ..., etc.\n",
        "def remove_stop_words(words):\n",
        "    result = [i for i in words if i not in ENGLISH_STOP_WORDS]\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# Reduce a word to its root word.\n",
        "def word_stemmer(words):\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    return [stemmer.stem(o) for o in words]\n",
        "\n",
        "\n",
        "# Remove inflectional endings only and to return the base.\n",
        "def word_lemmatizer(words):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    return [lemmatizer.lemmatize(o) for o in words]\n",
        "\n",
        "\n",
        "# Clear out the unnecessary information.\n",
        "def clean_token_pipeline(words):\n",
        "    cleaning_utils = [remove_stop_words, word_lemmatizer]\n",
        "\n",
        "    for o in cleaning_utils:\n",
        "        words = o(words)\n",
        "\n",
        "    return words\n",
        "\n",
        "\n",
        "def preprocess(X_train, X_val, X_test):\n",
        "    # Ensure all elements are strings\n",
        "    X_train = [str(x) if not isinstance(x, str) else x for x in X_train]\n",
        "    X_test = [str(x) if not isinstance(x, str) else x for x in X_test]\n",
        "    X_val = [str(x) if not isinstance(x, str) else x for x in X_val]\n",
        "\n",
        "    x_train_clean = [clean_up_pipeline(o) for o in X_train]\n",
        "    x_test_clean = [clean_up_pipeline(o) for o in X_test]\n",
        "    x_val_clean = [clean_up_pipeline(o) for o in X_val]\n",
        "\n",
        "    x_train_tokenize = [word_tokenize(o) for o in x_train_clean]\n",
        "    x_test_tokenize = [word_tokenize(o) for o in x_test_clean]\n",
        "    x_val_tokenize = [word_tokenize(o) for o in x_val_clean]\n",
        "\n",
        "    x_train_clean_token = [clean_token_pipeline(o) for o in x_train_tokenize]\n",
        "    x_test_clean_token = [clean_token_pipeline(o) for o in x_test_tokenize]\n",
        "    x_val_clean_token = [clean_token_pipeline(o) for o in x_val_tokenize]\n",
        "\n",
        "    x_train = [\" \".join(o) for o in x_train_clean_token]\n",
        "    x_test = [\" \".join(o) for o in x_test_clean_token]\n",
        "    x_val = [\" \".join(o) for o in x_val_clean_token]\n",
        "\n",
        "    return x_train, x_val, x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoPue9h3Ik--",
        "outputId": "da26a50e-101b-4f0e-c6ca-f3c562ff9368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "crossing boundary interdisciplinary approach latin america th june nd july paper international conference aim explore contemporary cultural debate taking place latin america draw various strand debate multidisciplinary forum paper consider various issue modernization hybridity transculturation apply various field study paper welcome following field cultural study literature particularly looking trend contemporary narrative including neoavantgarde popular fiction drama study cinema gender study popular culture comparative literature anthropology ethnography sociology linguistics economics politics law symposium proposed far include exile latin american experience indigenismo negrismo u s latin america paper longer minute abstract word english spanish portuguese sent preferably email conference organiser department language cultural study university limerick ireland st january conference organizer nuala finnegan kate quinn nancy serrano department language cultural study university limerick limerick ireland tel fax email nuala finnegan ul kate quinn ul nancy serrano ul update visit webpage http www ul neylonm conf html mr michele j neylon department language cultural study university limerick limerick ireland tel http www ul neylonm index html\n"
          ]
        }
      ],
      "source": [
        "x_train, x_val, x_test = preprocess(X_train, X_val, X_test)\n",
        "print(x_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N68YIZqIJmc8"
      },
      "source": [
        "## **3. Feature Extraction**\n",
        "In this step, we convert the words of an email into a numerical feature vector, representing information of that email used for classification. Used in this lab, TF-IDF stands for term frequency-inverse document frequency. It is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in information retrieval and text mining. The TF-IDF value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general.\n",
        "\n",
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlpwcsgxJrMK"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "\n",
        "def convert_to_feature(raw_tokenize_data):\n",
        "    raw_sentences = [' '.join(o) for o in raw_tokenize_data]\n",
        "\n",
        "    return vectorizer.transform(raw_sentences)\n",
        "\n",
        "\n",
        "def TfidfConvert(x_train, x_test, x_val):\n",
        "    x_train = [o.split(\" \") for o in x_train]\n",
        "    x_test = [o.split(\" \") for o in x_test]\n",
        "    x_val = [o.split(\" \") for o in x_val]\n",
        "\n",
        "    x_train_raw_sentences = [' '.join(o) for o in x_train]\n",
        "    x_val_raw_sentences = [' '.join(o) for o in x_val]\n",
        "    raw_sentences = x_train_raw_sentences + x_val_raw_sentences\n",
        "    vectorizer.fit(raw_sentences)\n",
        "\n",
        "    x_train_features = convert_to_feature(x_train)\n",
        "    x_test_features = convert_to_feature(x_test)\n",
        "    x_val_features = convert_to_feature(x_val)\n",
        "\n",
        "\n",
        "    return x_train_features, x_test_features, x_val_features\n",
        "\n",
        "\n",
        "def feature_extraction(x_train, x_test, x_val):\n",
        "    x_train_features, x_test_features, x_val_features = TfidfConvert(x_train, x_test, x_val)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "\n",
        "    return x_train_features, x_test_features, x_val_features, feature_names, vectorizer, 'NaN'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKUZs88oPQHI",
        "outputId": "e6cf6b1c-dfd9-479e-82c8-579c627ba583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  (0, 179)\t0.030829098200916682\n",
            "  (0, 925)\t0.03924121482935551\n",
            "  (0, 1447)\t0.12876127569162082\n",
            "  (0, 1450)\t0.036841375969293544\n",
            "  (0, 1910)\t0.05387576090798692\n",
            "  (0, 2145)\t0.04741317226216706\n",
            "  (0, 2178)\t0.032362735828499786\n",
            "  (0, 4976)\t0.052290213317405805\n",
            "  (0, 7018)\t0.06889003391908574\n",
            "  (0, 7757)\t0.044944552987890844\n",
            "  (0, 8080)\t0.07062861584874715\n",
            "  (0, 8084)\t0.08352747568137316\n",
            "  (0, 8237)\t0.04777899214937877\n",
            "  (0, 8372)\t0.10861920246225355\n",
            "  (0, 9079)\t0.06889003391908574\n",
            "  (0, 9244)\t0.22520682189228186\n",
            "  (0, 9247)\t0.04292042523054027\n",
            "  (0, 9698)\t0.10691598498062158\n",
            "  (0, 10128)\t0.08933019267341409\n",
            "  (0, 11519)\t0.07670766510999388\n",
            "  (0, 11528)\t0.05888262232227611\n",
            "  (0, 11963)\t0.07062861584874715\n",
            "  (0, 12488)\t0.04128930476493961\n",
            "  (0, 12788)\t0.024656223346770763\n",
            "  (0, 13387)\t0.06972585755200578\n",
            "  :\t:\n",
            "  (0, 33200)\t0.05011131420299052\n",
            "  (0, 33784)\t0.04616503533029908\n",
            "  (0, 34375)\t0.1725277564611268\n",
            "  (0, 37953)\t0.030967826367581493\n",
            "  (0, 38052)\t0.16100510602996707\n",
            "  (0, 39197)\t0.05649938163702828\n",
            "  (0, 39533)\t0.04349893448418048\n",
            "  (0, 39933)\t0.036935756919649114\n",
            "  (0, 40331)\t0.06889003391908574\n",
            "  (0, 40487)\t0.19504570471654953\n",
            "  (0, 41217)\t0.04694316851645876\n",
            "  (0, 41454)\t0.04828597447821518\n",
            "  (0, 41764)\t0.06769244120324834\n",
            "  (0, 42099)\t0.03044949888896026\n",
            "  (0, 42995)\t0.09083689932171275\n",
            "  (0, 43136)\t0.05387576090798692\n",
            "  (0, 43850)\t0.39223123519827635\n",
            "  (0, 44316)\t0.056522368318073966\n",
            "  (0, 44539)\t0.05387576090798692\n",
            "  (0, 44981)\t0.11181735690979103\n",
            "  (0, 45522)\t0.03977696789626131\n",
            "  (0, 46108)\t0.06738400574898834\n",
            "  (0, 46195)\t0.039476191198262005\n",
            "  (0, 46760)\t0.02633910907558818\n",
            "  (0, 46945)\t0.049959748369703114\n"
          ]
        }
      ],
      "source": [
        "x_train_features, x_test_features, x_val_features, feature_names, feature_model, scalar = feature_extraction(x_train, x_test, x_val)\n",
        "print(x_train_features[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MND-TjQRXkpq"
      },
      "source": [
        "### **Question 2**\n",
        "Please research TF-IDF online and provide a concise explanation of how it is done in your own words in one paragraph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0RZeCBFLSH9"
      },
      "source": [
        "## **4. Desiging a Function to Train and Test a SVM-based Spam Filter**\n",
        "The following function will train a Support Vector Machine (SVM) model as a spam filter. It also uses a validation dataset to evaluate how it performs.\n",
        "\n",
        "\n",
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAeBdHNfLM7k"
      },
      "outputs": [],
      "source": [
        "from secml.data import CDataset\n",
        "from secml.data.splitter import CDataSplitterKFold\n",
        "from secml.ml.classifiers import CClassifierSVM\n",
        "from secml.ml.peval.metrics import CMetricAccuracy\n",
        "from secml.ml.peval.metrics import CMetricConfusionMatrix\n",
        "from secml.adv.attacks.evasion import CAttackEvasionPGD\n",
        "# from Feature_extraction import single_transform\n",
        "import csv\n",
        "from statistics import mean, stdev\n",
        "import threading\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "# We use the training data and validation data set to train the SVM model.\n",
        "def train_SVM(x_train_features, x_val_features, y_train, y_val):\n",
        "    tr_set = CDataset(x_train_features, y_train)\n",
        "    # Train the SVM\n",
        "    print(\"Build SVM\")\n",
        "    xval_splitter = CDataSplitterKFold()\n",
        "    clf_lin = CClassifierSVM()\n",
        "    xval_lin_params = {'C': [1]}\n",
        "    print(\"Find the best params\")\n",
        "    best_lin_params = clf_lin.estimate_parameters(\n",
        "        dataset=tr_set,\n",
        "        parameters=xval_lin_params,\n",
        "        splitter=xval_splitter,\n",
        "        metric='accuracy',\n",
        "        perf_evaluator='xval'\n",
        "    )\n",
        "    print(\"Finish Train\")\n",
        "    print(\"The best training parameters are: \", [\n",
        "          (k, best_lin_params[k]) for k in sorted(best_lin_params)])\n",
        "    print(\"Train SVM\")\n",
        "    clf_lin.fit(tr_set.X, tr_set.Y)\n",
        "\n",
        "    # Test the Classifier\n",
        "    v_set = CDataset(x_val_features, y_val)\n",
        "    y_pred = clf_lin.predict(v_set.X)\n",
        "    metric = CMetricAccuracy()\n",
        "    acc = metric.performance_score(y_true=v_set.Y, y_pred=y_pred)\n",
        "    confusion_matrix = CMetricConfusionMatrix()\n",
        "    cm = confusion_matrix.performance_score(y_true=v_set.Y, y_pred=y_pred)\n",
        "    print(\"Confusion Matrix: \")\n",
        "    print(cm)\n",
        "\n",
        "\n",
        "    return tr_set, v_set, clf_lin\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rKjzZTCO5N2"
      },
      "source": [
        "## **5. Conducting PGD Attacks Using Randomly Selected Spam Emails**\n",
        "Adversarial perturbations are made to input features, i.e., the TF-IDF values corresponding to words. Here, we employ the Projected Gradient Descent (PGD) method to modify the TF-IDF features. The PGD algorithm iteratively finds the needed changes with a constraint parameter, *dmax*, which is the Euclidean distance to the original input indicating the allowed extent of perturbation, to achieve the maximum loss in classification. In our approach, we run PGD over a set of spam emails randomly selected from the validation sataset to generate adversarialspam examples in the feature space. Then we test these modified TF-IDF vectors to see whether they could successfully bypass the detection.\n",
        "\n",
        "**First we train a SVM-based spam filter and prepare the validation dataset for the PGD attack later:**\n",
        "\n",
        "This step also tests the performance of this classifier on the validation dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bi5HMdEVQ3a4",
        "outputId": "1a9e31b2-1abc-4e4b-f20f-3bec1e900969"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Build SVM\n",
            "Find the best params\n",
            "Finish Train\n",
            "The best training parameters are:  [('C', 1)]\n",
            "Train SVM\n",
            "Confusion Matrix: \n",
            "CArray([[386   0]\n",
            " [  6  71]])\n"
          ]
        }
      ],
      "source": [
        "tr_set, v_set, clf_lin = train_SVM(x_train_features, x_val_features, Y_train, Y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix7GUWE6yAxQ"
      },
      "source": [
        "**Then run the code block below of a function to do PGD attack on a spam filter:**\n",
        "\n",
        "**Note:**\n",
        "These are explanations for the terms used in the code:\n",
        "\n",
        "1. `clf_lin` - SVM Classifier:\n",
        "   - Represents a Support Vector Machine (SVM) classifier for email classification.\n",
        "\n",
        "2. `tr_set` - Training Set:\n",
        "   - A dataset used for training the SVM classifier, containing input features and labels.\n",
        "\n",
        "3. `v_set` - Validation Set:\n",
        "   - A dataset used to identify the \"best\" magic words through PGD.\n",
        "\n",
        "4. `Y_val` - Validation Set Labels:\n",
        "   - Contains labels for the validation set, aiding in performance evaluation.\n",
        "\n",
        "5. `feature_names` - Name of the Features:\n",
        "   - Likely holds the names or labels of the dataset's features.\n",
        "\n",
        "6. `nb_attack` - Number of Attacks:\n",
        "   - Determines how many adversarial examples should be generated (the number of spam emails to modify by PGD).\n",
        "\n",
        "7. `dmax` - Maximum Perturbation Distance:\n",
        "   - Represents the maximum allowed change in feature values during adversarial attacks, measured as Euclidean distance.\n",
        "\n",
        "8. `lb` - Lower Bound:\n",
        "   - Sets a lower boundary on feature values, constraining perturbations during attacks.\n",
        "\n",
        "9. `ub` - Upper Bound:\n",
        "   - Defines an upper boundary on feature values, limiting perturbations during attacks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCvPfWA-P1OG"
      },
      "outputs": [],
      "source": [
        "def pgd_attack(clf_lin, tr_set, v_set, y_val, feature_names, nb_attack, dmax, lb, ub):\n",
        "\n",
        "    class_to_attack = 1\n",
        "    cnt = 0  # the number of success adversaril examples\n",
        "    ori_examples2_x = []\n",
        "    ori_examples2_y = []\n",
        "\n",
        "    for i in range(nb_attack):\n",
        "        # take a point at random being the starting point of the attack\n",
        "        idx_candidates = np.where(y_val == class_to_attack)\n",
        "        # select nb_init_pts points randomly in candidates and make them move\n",
        "        rn = np.random.choice(idx_candidates[0].size, 1)\n",
        "        x0, y0 = v_set[idx_candidates[0][rn[0]], :].X, v_set[idx_candidates[0][rn[0]], :].Y\n",
        "\n",
        "        x0 = x0.astype(float)\n",
        "        y0 = y0.astype(int)\n",
        "        x2 = x0.tondarray()[0]\n",
        "        y2 = y0.tondarray()[0]\n",
        "\n",
        "        ori_examples2_x.append(x2)\n",
        "        ori_examples2_y.append(y2)\n",
        "\n",
        "\n",
        "    # Perform adversarial attacks\n",
        "\n",
        "    noise_type = 'l2'  # Type of perturbation 'l1' or 'l2'\n",
        "\n",
        "    y_target = 0\n",
        "\n",
        "    # dmax = 0.09  # Maximum perturbation\n",
        "\n",
        "    # Bounds of the attack space. Can be set to `None` for unbounded\n",
        "    solver_params = {\n",
        "        'eta': 0.01,\n",
        "        'max_iter': 1000,\n",
        "        'eps': 1e-4}\n",
        "\n",
        "    # set lower bound and upper bound respectively to 0 and 1 since all features are Boolean\n",
        "    pgd_attack = CAttackEvasionPGD(\n",
        "        classifier=clf_lin,\n",
        "        double_init_ds=tr_set,\n",
        "        distance=noise_type,\n",
        "        dmax=dmax,\n",
        "        lb=lb, ub=ub,\n",
        "        solver_params=solver_params,\n",
        "        y_target=y_target\n",
        "    )\n",
        "\n",
        "    ad_examples_x = []\n",
        "    ad_examples_y = []\n",
        "    ad_index = []\n",
        "    cnt = 0\n",
        "\n",
        "    for i in range(len(ori_examples2_x)):\n",
        "        x0 = ori_examples2_x[i]\n",
        "        y0 = ori_examples2_y[i]\n",
        "        y_pred_pgd, _, adv_ds_pgd, _ = pgd_attack.run(x0, y0)\n",
        "\n",
        "        if y_pred_pgd.item() == 0:\n",
        "            cnt = cnt + 1\n",
        "            ad_index.append(i)\n",
        "\n",
        "        ad_examples_x.append(adv_ds_pgd.X.tondarray()[0])\n",
        "        ad_examples_y.append(y_pred_pgd.item())\n",
        "\n",
        "        attack_pt = adv_ds_pgd.X.tondarray()[0]\n",
        "\n",
        "    print(\"PGD attack successful rate:\", cnt / nb_attack)\n",
        "\n",
        "    startTime2 = time.time()\n",
        "    ori_examples2_x = np.array(ori_examples2_x)\n",
        "    ori_examples2_y = np.array(ori_examples2_y)\n",
        "    ad_examples_x = np.array(ad_examples_x)\n",
        "    ad_examples_y = np.array(ad_examples_y)\n",
        "\n",
        "    ori_dataframe = pd.DataFrame(ori_examples2_x, columns=feature_names)\n",
        "    ad_dataframe = pd.DataFrame(ad_examples_x, columns=feature_names)\n",
        "\n",
        "    # extract the success and fail examples\n",
        "\n",
        "    ad_dataframe['ad_label'] = ad_examples_y\n",
        "    ad_success = ad_dataframe.loc[ad_dataframe.ad_label == 0]\n",
        "    ori_success = ori_dataframe.loc[ad_dataframe.ad_label == 0]\n",
        "    ad_fail = ad_dataframe.loc[ad_dataframe.ad_label == 1]\n",
        "    ori_fail = ori_dataframe.loc[ad_dataframe.ad_label == 1]\n",
        "\n",
        "    ad_success_x = ad_success.drop(columns=['ad_label'])\n",
        "    ad_fail_x = ad_fail.drop(columns=['ad_label'])\n",
        "\n",
        "    result = (ad_success_x - ori_success)\n",
        "    ori_dataframe.to_csv('ori_dataframe.csv')\n",
        "    ad_dataframe.to_csv('ad_dataframe.csv')\n",
        "    result.to_csv('result.csv')\n",
        "\n",
        "\n",
        "\n",
        "    return result, cnt, ad_success_x, ori_dataframe, ori_examples2_y, cnt/nb_attack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3qmM8uxrzrg"
      },
      "source": [
        "**Now we can call this function to attack the trained spam filter:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RYKIvm0QMHD",
        "outputId": "e4b53c9e-3355-4591-c3b0-4546cb5c0468"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PGD attack successful rate: 0.25\n"
          ]
        }
      ],
      "source": [
        "lb = np.ndarray.min(x_train_features.toarray())\n",
        "ub = np.ndarray.max(x_train_features.toarray())\n",
        "attack_amount = 100\n",
        "dmax = 0.06\n",
        "result, cnt, ad_success_x, ori_dataframe, ori_examples2_y, successful_rate = pgd_attack(clf_lin, tr_set, v_set, Y_val, feature_names, attack_amount, dmax, lb, ub)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf2zyWXxnsxx"
      },
      "source": [
        "### **Question 3**\n",
        "Please explain how the PGD success rate is calculated based on the code above using your own words. Then compare this success rate to the original false negative rate of this classifier on the validation set. What do you find?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xUx6FBQiCBr"
      },
      "source": [
        "## **6. Identifying Magical Words**\n",
        "Adversarial emails are crafted by adding “magic words” to the original spam emails. The “magic words” are identified by intersecting the unique ham words with the “top words”. Specifically,  the unique ham words only appear in ham emails but not in spam emails.  After the  PGD  attacks on the set of randomly selected spam emails, we examine statistically which features are modified to the largest extent in the effort to bypass detection, to find their corresponding “top words”. (The changes are measured by calculating the variance of TF-IDF differences ver these spam emails before and after the PGD perturbation.) In  our  experiments,  we  use  the  top 100  words,  which  is relatively efficient. This set is relatively small and effective.\n",
        "\n",
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mc552xOTSQZn"
      },
      "outputs": [],
      "source": [
        "def magical_word(x_train, x_val, y_train, y_val, result, cnt):\n",
        "\n",
        "    # Method 2: calculate feature importance\n",
        "    result_array = np.array(result)\n",
        "    weighted_result = result.multiply(result_array)\n",
        "\n",
        "    average_importance = weighted_result.sum() / cnt\n",
        "    average_importance_df = pd.DataFrame(average_importance, columns=['importance'])\n",
        "    sorted_features = average_importance_df.sort_values(by='importance', ascending=False, inplace=False)\n",
        "\n",
        "    important_features_df = pd.DataFrame(sorted_features.index[:100])\n",
        "    important_features_df.to_csv(\"important_features.csv\")\n",
        "\n",
        "    # Combine train and validation datasets\n",
        "    train_data = pd.DataFrame({'message': x_train, 'label': y_train})\n",
        "    val_data = pd.DataFrame({'message': x_val, 'label': y_val})\n",
        "    combined_data = pd.concat([train_data, val_data])\n",
        "    combined_data.to_csv(\"combined_messages.csv\")\n",
        "\n",
        "    # Separate spam and ham messages\n",
        "    spam_messages = combined_data[combined_data.label == 1]\n",
        "    ham_messages = combined_data[combined_data.label == 0]\n",
        "\n",
        "    # # Save test messages\n",
        "    # test_data = pd.DataFrame({'message': x_test, 'label': y_test})\n",
        "    # test_data.to_csv(\"test_messages.csv\")\n",
        "    # spam_test_messages = test_data[test_data.label == 1]\n",
        "\n",
        "    # Tf-idf for spam datasets\n",
        "    tfidf_vectorizer_spam = TfidfVectorizer()\n",
        "    tfidf_vectorizer_spam.fit_transform(spam_messages['message'])\n",
        "    spam_feature_names = tfidf_vectorizer_spam.get_feature_names_out()\n",
        "\n",
        "    # Tf-idf for ham datasets\n",
        "    tfidf_vectorizer_ham = TfidfVectorizer()\n",
        "    tfidf_vectorizer_ham.fit_transform(ham_messages['message'])\n",
        "    ham_feature_names = tfidf_vectorizer_ham.get_feature_names_out()\n",
        "\n",
        "    # find unique ham words\n",
        "    unique_ham_words = list(set(ham_feature_names) - set(spam_feature_names))\n",
        "    unique_ham_words_df = pd.DataFrame(unique_ham_words)\n",
        "    unique_ham_words_df.to_csv(\"unique_ham_words.csv\")\n",
        "\n",
        "    with open(\"important_features.csv\", \"r\") as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        important_features = [row[1] for row in reader]\n",
        "\n",
        "    important_features = important_features[1:]\n",
        "    # in ham & top100\n",
        "\n",
        "    ham_words_in_important_features = list(set(unique_ham_words).intersection(set(important_features)))\n",
        "    ham_words_str = \" \".join(ham_words_in_important_features)\n",
        "\n",
        "    return ham_words_str, spam_messages, ham_messages\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjldI2rTs_N5"
      },
      "source": [
        "**Run the code block below to list the magic words:**\n",
        "\n",
        "Note that Variable 'identified_magic_words' contains the identified magic words. These words will be used in the attacks later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yC_GNgts8UO",
        "outputId": "4652a35e-2748-4085-b88c-3ae25293db56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "arizona chorus phonetic theory linguist cascadilla workshop academic native benjamin grammar translation linguistic euralex proceeding ldc sentence risked ammondt french pkzip ipa colingacl\n",
            "23\n"
          ]
        }
      ],
      "source": [
        "identified_magic_words, spam, ham = magical_word(x_train, x_val, Y_train, Y_val, result, cnt)\n",
        "print(identified_magic_words)\n",
        "print(len(identified_magic_words.split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0U_2SVI4sNK9"
      },
      "outputs": [],
      "source": [
        "# save your magic words\n",
        "with open('magic_words.txt', 'w') as f:\n",
        "    f.write(identified_magic_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgS8EWh6qwSl"
      },
      "source": [
        "### **Question 4**\n",
        "(1). Based on your understanding after reading the paper using the link below, explain what is a \"good word\" attack?\n",
        "(Reference: https://www.ceas.cc/papers-2005/125.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulr-4hJpysdz"
      },
      "source": [
        "(2). The use of the good words is similar to our \"magic words\" in this approach.\n",
        "What is the difference in finding \"magic words\" from finding \"good words\"?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9e3VAwouiQK"
      },
      "source": [
        "## **7. Crafting Adversarial Emails by Adding Magic Words and Attacking a Spam Filter**\n",
        "After we find the magical words, we then add them to a spam email to evaluate how effectively the \"magic words\" can bypass the classifier. This is what we called \"crafting adversarial emails\". Then, we feed the new TF-IDF vectors of these crafted emails to the SVM classifier to see if they would be misclassified as ham emails.\n",
        "\n",
        "**Run the code block below:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxY8tJakw2yv"
      },
      "outputs": [],
      "source": [
        "m2_empty = pd.DataFrame()\n",
        "spam_cnt = 0\n",
        "threads = []\n",
        "m2_empty_l1 = pd.DataFrame()\n",
        "m2_empty_l2 = pd.DataFrame()\n",
        "m2_empty_l3 = pd.DataFrame()\n",
        "m2_empty_l4 = pd.DataFrame()\n",
        "m2_list = [m2_empty_l1, m2_empty_l2, m2_empty_l3, m2_empty_l4]\n",
        "\n",
        "\n",
        "def single_transform(x, method, feature_model, feature_names, scalar, selection_model):\n",
        "\n",
        "  result = feature_model.transform(x)\n",
        "\n",
        "  if selection_model != 'NaN':\n",
        "    result = selection_model.transform(result)\n",
        "\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "class myThread(threading.Thread):\n",
        "\n",
        "    def __init__(self, threadID, name, spam_message, identified_magic_words, method, feature_model, feature_names, scalar, clf_lin, list_index, selection_model):\n",
        "        threading.Thread.__init__(self)\n",
        "        self.threadID = threadID\n",
        "        self.name = name\n",
        "        self.spam_message = spam_message\n",
        "        self.identified_magic_words = identified_magic_words\n",
        "        self.method = method\n",
        "        self.feature_model = feature_model\n",
        "        self.feature_names = feature_names\n",
        "        self.scalar = scalar\n",
        "        self.clf_lin = clf_lin\n",
        "        self.list_index = list_index\n",
        "        self.lock = threading.Lock()\n",
        "        self.selection_model = selection_model\n",
        "\n",
        "    def run(self):\n",
        "        global spam_cnt\n",
        "        print(\"Starting \" + self.name)\n",
        "        spam_cnt_1 = m2_empty_out(self.name, self.spam_message, self.identified_magic_words, self.method,\n",
        "                                  self.feature_model, self.feature_names, self.scalar, self.clf_lin,\n",
        "                                  self.list_index, self.selection_model)\n",
        "        spam_cnt = spam_cnt+spam_cnt_1\n",
        "        time.sleep(0.1)\n",
        "        print(\"Exiting \" + self.name)\n",
        "\n",
        "\n",
        "\n",
        "def m2_empty_out(name, spam_message, identified_magic_words, method, feature_model, feature_names, scalar, clf_lin, list_index, selection_model):\n",
        "    m2_empty_1 = pd.DataFrame()\n",
        "    spam_cnt_1 = 0\n",
        "    global m2_list\n",
        "\n",
        "    for j in spam_message.message:\n",
        "        choose_email = [j + identified_magic_words]\n",
        "        message_14_email = pd.DataFrame(choose_email, columns=[\"message\"])\n",
        "        message_14_tf_idf = single_transform(\n",
        "            message_14_email[\"message\"], method, feature_model, feature_names, scalar, selection_model)\n",
        "        message_14_tf_idf = pd.DataFrame(\n",
        "            message_14_tf_idf.toarray(), columns=feature_names)\n",
        "        message_14_y = [1]\n",
        "        message_14_y = pd.Series(message_14_y)\n",
        "        message_CData = CDataset(message_14_tf_idf, message_14_y)\n",
        "        message_14_pred = clf_lin.predict(message_CData.X)\n",
        "\n",
        "        if message_14_pred == 0:\n",
        "            spam_cnt_1 += 1\n",
        "            m2_empty_1 = pd.concat([m2_empty_1, message_14_tf_idf], ignore_index=True)\n",
        "\n",
        "    m2_list[list_index] = pd.concat([m2_list[list_index], m2_empty_1], ignore_index=True)\n",
        "\n",
        "    return spam_cnt_1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def svm_attack(method, clf_lin, spam, identified_magic_words, feature_model, feature_names, scalar, selection_model):\n",
        "\n",
        "    global m2_empty\n",
        "\n",
        "    spam_messages = np.array_split(spam, 4)\n",
        "\n",
        "    print(\"Start processing message\")\n",
        "\n",
        "    thread1 = myThread(1, \"Thread-1\", spam_messages[0], identified_magic_words,\n",
        "                       method, feature_model, feature_names, scalar, clf_lin, 0, selection_model)\n",
        "    thread2 = myThread(2, \"Thread-2\", spam_messages[1], identified_magic_words,\n",
        "                       method, feature_model, feature_names, scalar, clf_lin, 1, selection_model)\n",
        "    thread3 = myThread(3, \"Thread-3\", spam_messages[2], identified_magic_words,\n",
        "                       method, feature_model, feature_names, scalar, clf_lin, 2, selection_model)\n",
        "    thread4 = myThread(4, \"Thread-4\", spam_messages[3], identified_magic_words,\n",
        "                       method, feature_model, feature_names, scalar, clf_lin, 3, selection_model)\n",
        "\n",
        "    threads.append(thread1)\n",
        "    threads.append(thread2)\n",
        "    threads.append(thread3)\n",
        "    threads.append(thread4)\n",
        "\n",
        "    for t in threads:\n",
        "        t.start()\n",
        "\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "\n",
        "    m2_empty = pd.concat(m2_list, ignore_index=True)\n",
        "\n",
        "    print(\"Exiting Main Thread\")\n",
        "    print('White box attack on SVM:')\n",
        "    print('Number of samples provided:', len(spam))\n",
        "    print('Number of crafted sample that got misclassified:', spam_cnt)\n",
        "    print('Successful rate:', spam_cnt / len(spam))\n",
        "\n",
        "    return m2_empty\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we combine the training and validation data to train a SVM-based spam filter as the target classifier and test its performance using the testing dataset. The information is needed in analyzing the effectiveness of adversarial attacks by adding magic words."
      ],
      "metadata": {
        "id": "NLIjWiQm9CFz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc4zoOL4s-H0",
        "outputId": "b2ada0df-7abe-431c-e4f4-b9075b0f624e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Build SVM\n",
            "Find the best params\n",
            "Finish Train\n",
            "The best training parameters are:  [('C', 1)]\n",
            "Train SVM\n",
            "Confusion Matrix: \n",
            "CArray([[482   1]\n",
            " [  6  90]])\n"
          ]
        }
      ],
      "source": [
        "# Comnbine the datsets and train a classifer\n",
        "combined_data = pd.read_csv(\"combined_messages.csv\")\n",
        "x_combined = combined_data['message']\n",
        "Y_combined = combined_data['label']\n",
        "x_combined = [str(x) if not isinstance(x, str) else x for x in x_combined]\n",
        "x_combined = [o.split(\" \") for o in x_combined]\n",
        "x_combined_features = convert_to_feature(x_combined)\n",
        "\n",
        "tr_set, test_set, clf_lin_combined = train_SVM(x_combined_features, x_test_features, Y_combined, Y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jpmgPJWa6C4H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL1ZOAYQN12a"
      },
      "source": [
        "### **Question 5**\n",
        "Based on the confusion matrix provided above, calculate the following metrics: `accuracy`, `false-positive rate`, `false-negative rate`, `true-positive rate`, and `true-negative rate`. Please outline the steps you took to calculate each metric and include your results in the text block below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We extract spam emails from the testing dataset for adversarial attacks using the identified magic words:**"
      ],
      "metadata": {
        "id": "28KKscxD47fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.DataFrame({'message': x_test, 'label': Y_test})\n",
        "spam_test = test_data[test_data.label == 1]"
      ],
      "metadata": {
        "id": "D5cKOQIH4KzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raxGc9-UsT6d"
      },
      "source": [
        "**Run the code block below:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo5YVW5zw0Iu",
        "outputId": "4e1633a3-741c-4e7e-a8a4-9dcda8d2e632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start processing message\n",
            "Starting Thread-1\n",
            "Starting Thread-2\n",
            "Starting Thread-3\n",
            "Starting Thread-4\n",
            "Exiting Thread-1\n",
            "Exiting Thread-3\n",
            "Exiting Thread-4\n",
            "Exiting Thread-2\n",
            "Exiting Main Thread\n",
            "White box attack on SVM:\n",
            "Number of samples provided: 96\n",
            "Number of crafted sample that got misclassified: 45\n",
            "Successful rate: 0.46875\n"
          ]
        }
      ],
      "source": [
        "m2_empty = svm_attack('TFIDF', clf_lin_combined, spam_test, identified_magic_words, feature_model, feature_names, scalar, 'NaN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQOQYRvy4QBG"
      },
      "source": [
        "### **Question 6**\n",
        "Is the success rate shown above higher or lower than the false negative rate of the target classifier? Based on this comparison, can we determine if the attack is effective?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}